03/19/2025 21:18:27 - INFO - __main__ - ***** Training arguments *****
03/19/2025 21:18:27 - INFO - __main__ - Namespace(config='configs/ddpm.yaml', data_dir='../imagenet100_128x128/train', image_size=128, batch_size=32, num_workers=8, num_classes=100, run_name='exp-1-ddpm', output_dir='experiments', num_epochs=100, learning_rate=0.0001, weight_decay=1e-05, grad_clip=1.0, seed=42, mixed_precision='none', num_train_timesteps=1000, num_inference_steps=100, beta_start=0.0001, beta_end=0.02, beta_schedule='linear', variance_type='fixed_small', prediction_type='epsilon', clip_sample=True, clip_sample_range=1.0, unet_in_size=128, unet_in_ch=3, unet_ch=128, unet_ch_mult=[1, 2, 2, 4], unet_attn=[2, 3], unet_num_res_blocks=2, unet_dropout=0.0, latent_ddpm=False, use_cfg=False, cfg_guidance_scale=2.0, use_ddim=False, ckpt=None, max_train_steps=406200, distributed=False, world_size=1, rank=0, local_rank=0, device='cuda', total_batch_size=32)
03/19/2025 21:18:27 - INFO - __main__ - ***** Running training *****
03/19/2025 21:18:27 - INFO - __main__ -   Num examples = 130000
03/19/2025 21:18:27 - INFO - __main__ -   Num Epochs = 100
03/19/2025 21:18:27 - INFO - __main__ -   Instantaneous batch size per device = 32
03/19/2025 21:18:27 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
03/19/2025 21:18:27 - INFO - __main__ -   Total optimization steps per epoch 4062
03/19/2025 21:18:27 - INFO - __main__ -   Total optimization steps = 406200
  0%|                                                      | 0/406200 [00:00<?, ?it/s]03/19/2025 21:18:27 - INFO - __main__ - Epoch 1/100
/jet/home/yzhou17/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
  0%|                                          | 1/406200 [00:02<280:01:47,  2.48s/it]03/19/2025 21:18:29 - INFO - __main__ - Epoch 1/100, Step 0/4062, Loss 0.9970714449882507 (0.9970714449882507)
  0%|                                        | 101/406200 [01:52<124:07:09,  1.10s/it]03/19/2025 21:20:19 - INFO - __main__ - Epoch 1/100, Step 100/4062, Loss 0.36044520139694214 (0.7089588544746437)
  0%|                                        | 141/406200 [02:36<124:06:58,  1.10s/it]Traceback (most recent call last):
  File "/jet/home/yzhou17/hw5/hw5_student_starter_code/train.py", line 438, in <module>
    main()
  File "/jet/home/yzhou17/hw5/hw5_student_starter_code/train.py", line 373, in main
    loss_m.update(loss.item())
KeyboardInterrupt
